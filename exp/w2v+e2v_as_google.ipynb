{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for training e2v along with w2v using method which Google w2v was trained with. Read more about it: https://www.tensorflow.org/tutorials/word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "import pickle as pk\n",
    "import gensim.models as gs\n",
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "# Internal dependencies\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from model import Emoji2Vec\n",
    "from trainer import Trainer\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import math\n",
    "import zipfile\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "\n",
    "from tfnn.layers import *\n",
    "import tfnn\n",
    "import math\n",
    "import mt.bleu\n",
    "import sys\n",
    "import numpy as np\n",
    "import collections\n",
    "from mt.strutils import tokenize\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emoji_minibatch_len = 16\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read the data into a list of strings.\n",
    "def read_data(filename):\n",
    "    \"\"\"Extract the first file enclosed in a zip file as a list of words.\"\"\"\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary = read_data('text8.zip')\n",
    "print('Data size', len(vocabulary))\n",
    "words_voc_size = 80000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "e2v = {}\n",
    "w2v = {}\n",
    "with open(\"./e2v_full.txt\", \"r\") as f:\n",
    "    for i, line in enumerate(f.readlines()):\n",
    "        line = line.split('\\t')\n",
    "        e2v[line[0]] = [float(x) for x in line[1].split()]\n",
    "with open(\"./w2v_full.txt\", \"r\") as f:\n",
    "    for i, line in enumerate(f.readlines()):\n",
    "        line = line.split('\\t')\n",
    "        w2v[line[0]] = [float(x) for x in line[1].split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dataset(words, n_words):\n",
    "    \"\"\"Process raw inputs into a dataset.\"\"\"\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(n_words - 1))\n",
    "    keys = [x[0] for x in count]\n",
    "    only_in_w2v = [word for word in w2v if word not in keys]\n",
    "    index = 0\n",
    "    for i in range(len(count)):\n",
    "        if count[i][0] not in w2v:\n",
    "            count[i] = (only_in_w2v[index], 4)\n",
    "            index += 1\n",
    "    dictionary = dict()\n",
    "    extended_dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  # dictionary['UNK']\n",
    "            unk_count += 1\n",
    "        if word in extended_dictionary:\n",
    "            extended_dictionary[word].append(len(data))\n",
    "        else:\n",
    "            extended_dictionary[word] = [len(data)]\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary, extended_dictionary\n",
    "\n",
    "data, count, dictionary_w2v, reverse_dictionary_w2v, extended_dictionary = build_dataset(vocabulary,\n",
    "                                                            words_voc_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', data[:10], [reverse_dictionary_w2v[i] for i in data[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_data(datafile):\n",
    "    \n",
    "    def normalize_word(word):\n",
    "        alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "        word = word.lower()\n",
    "        try:\n",
    "            while word[0] not in alphabet:\n",
    "                word = word[1:]\n",
    "            while word[-1] not in alphabet:\n",
    "                word = word[:-1]\n",
    "        except:\n",
    "            return None\n",
    "        if word not in dictionary_w2v:\n",
    "            print(word)\n",
    "            return None\n",
    "        return word\n",
    "    \n",
    "    smiles_to_words = {} \n",
    "    emojis = []\n",
    "    words_from_emojis = []\n",
    "    with open(datafile, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            try:\n",
    "                em, phrase, truth = line.rstrip().split('\\t')\n",
    "            except:\n",
    "                print(em, phrase)\n",
    "                continue\n",
    "            phrase = phrase.lower()\n",
    "            new_words = [normalize_word(new_word) for new_word in phrase.split() if not normalize_word(new_word) is None]\n",
    "            if em in smiles_to_words:\n",
    "                smiles_to_words[em].extend(new_words)\n",
    "            else:\n",
    "                smiles_to_words[em] = new_words\n",
    "            emojis.append(em)\n",
    "            words_from_emojis.extend(new_words)\n",
    "    for key, value in smiles_to_words.items():\n",
    "        normalized_words = []\n",
    "        for word in list(set(value)):\n",
    "            norm_word = normalize_word(word)\n",
    "            if not norm_word is None:\n",
    "                normalized_words.append(norm_word)\n",
    "        smiles_to_words[key] = normalized_words\n",
    "    emojis = list(set(emojis))\n",
    "    words_from_emojis = list(set(words_from_emojis))\n",
    "    return smiles_to_words, emojis, words_from_emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "smiles_to_words, emojis, words_from_emojis = make_data('../data/emojipedia_positive.txt')\n",
    "data = [x+len(emojis) for x in data]\n",
    "dictionary_w2v = {k:v+len(emojis) for k, v in dictionary_w2v.items()}\n",
    "emojis_to_ind = {emojis[i]: i for i in range(len(emojis))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reverse_dictionary = []\n",
    "reverse_dictionary.extend(emojis)\n",
    "reverse_dictionary.extend(list(reverse_dictionary_w2v.values())[:words_voc_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emoji_voc_size = len(emojis)\n",
    "vocabulary_size = emoji_voc_size + words_voc_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "smiles = list(smiles_to_words.keys())\n",
    "smile_ind = 0\n",
    "word_window = 2\n",
    "word_w2v_ind = 2\n",
    "samples_per_word = 2\n",
    "samples_per_emoji = 4\n",
    "\n",
    "def get_emoji_minibatch():\n",
    "    global smile_ind\n",
    "    minibatch = []\n",
    "    \n",
    "    for i in range(emoji_minibatch_len // samples_per_emoji):\n",
    "        curr_words = smiles_to_words[smiles[smile_ind]]\n",
    "        while len(curr_words) < 1:\n",
    "            smile_ind += 1\n",
    "            smile_ind %= len(smiles)\n",
    "            curr_words = smiles_to_words[smiles[smile_ind]]\n",
    "            curr_words = [word for word in curr_words if len(word)>=3 and word not in [\"the\", \"this\", \"that\", \"are\"]]\n",
    "        smile = smiles[smile_ind]\n",
    "        \n",
    "        targets_to_awoid = []\n",
    "        for k in range(samples_per_emoji // samples_per_word):\n",
    "            word_ind = random.randint(0, len(curr_words)-1)\n",
    "            while word_ind in targets_to_awoid:\n",
    "                word_ind += 1\n",
    "                word_ind %= len(curr_words)\n",
    "\n",
    "            num_labels_added = 0\n",
    "            labels = extended_dictionary[curr_words[word_ind]]\n",
    "            for j in range(samples_per_word):   \n",
    "                label_ind = random.randint(0, len(labels)-1)\n",
    "                label = labels[label_ind]\n",
    "                \n",
    "                label_ind = random.randint(0, 5)\n",
    "                while label_ind == 2:\n",
    "                    label_ind = random.randint(0, 5)\n",
    "                label_ind = min(max(label+label_ind-2, 0), len(data)-1)\n",
    "                \n",
    "                what_first = random.randint(0, 2)\n",
    "                if what_first == 1:\n",
    "                    minibatch.append((emojis_to_ind[smile], dictionary_w2v[reverse_dictionary_w2v[data[label_ind]-len(emojis)]])) \n",
    "                else:\n",
    "                    minibatch.append((dictionary_w2v[reverse_dictionary_w2v[data[label_ind]-len(emojis)]], emojis_to_ind[smile]))\n",
    "                num_labels_added += 1\n",
    "                if num_labels_added == samples_per_word:\n",
    "                    break\n",
    "            targets_to_awoid.append(word_ind)\n",
    "            if len(targets_to_awoid) == len(curr_words):\n",
    "                break\n",
    "        smile_ind += 1\n",
    "        if smile_ind >= len(smiles):\n",
    "            smile_ind = 0\n",
    "    return minibatch\n",
    "\n",
    "def get_words_minibatch():\n",
    "    global word_window\n",
    "    global word_w2v_ind\n",
    "    minibatch = []\n",
    "    for i in range((batch_size-emoji_minibatch_len) // samples_per_word):\n",
    "        targets_to_awoid = [word_w2v_ind]\n",
    "        for j in range(samples_per_word):\n",
    "            next_word_ind = random.randint(-2, 2)\n",
    "            while next_word_ind in targets_to_awoid:\n",
    "                next_word_ind = random.randint(-2, 2)\n",
    "            word = reverse_dictionary_w2v[data[next_word_ind+word_w2v_ind]-len(emojis)]\n",
    "            targets_to_awoid.append(next_word_ind)\n",
    "            minibatch.append((data[word_w2v_ind], data[word_w2v_ind+next_word_ind])) \n",
    "        word_w2v_ind += 1\n",
    "        if word_w2v_ind >= len(data)-2:\n",
    "            word_w2v_ind = 2\n",
    "    return minibatch\n",
    "\n",
    "def get_minibatch():\n",
    "    minibatch = get_emoji_minibatch()\n",
    "    minibatch.extend(get_words_minibatch())\n",
    "    return minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get_minibatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_batch():\n",
    "    batch = []\n",
    "    while len(batch) < batch_size:\n",
    "        batch.extend(get_minibatch())\n",
    "    random.shuffle(batch)\n",
    "    return [batch[i][0] for i in range(batch_size)], [[batch[i][1]] for i in range(batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#generate_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_size = 300\n",
    "valid_size = 32     # Random set of words to evaluate similarity on.\n",
    "valid_examples = list(np.random.choice(len(smiles), valid_size // 2, replace=False))\n",
    "words_val_indices = np.random.choice(500, valid_size // 2, replace=False)\n",
    "valid_examples.extend([x+len(smiles) for x in words_val_indices])\n",
    "num_sampled = 64    # Number of negative examples to sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# True if some checkpoint already exists, False if you want to start over\n",
    "begin_from_checkpoint = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_embeddings = []\n",
    "if begin_from_checkpoint:\n",
    "    for emoji in emojis:\n",
    "        full_embeddings.append(e2v[emoji])\n",
    "    for i in range(len(emojis), len(reverse_dictionary)):\n",
    "        try:\n",
    "            full_embeddings.append(w2v[reverse_dictionary[i]])\n",
    "        except Exception as e:\n",
    "            print(reverse_dictionary[i])\n",
    "else:\n",
    "    for i in range(words_voc_size + len(emojis)):\n",
    "        full_embeddings.append([random.uniform(-1, 1) for i in range(embedding_size)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.array(full_embeddings).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data.\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "    \n",
    "    with tf.device('/cpu:0'):\n",
    "        embeddings = tf.Variable(np.array(full_embeddings), dtype=tf.float32)\n",
    "        embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "        \n",
    "        # Construct the variables for the NCE loss\n",
    "        nce_weights = tf.Variable(\n",
    "            tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                                stddev=1.0 / math.sqrt(embedding_size)))\n",
    "        nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "        \n",
    "        # Compute the average NCE loss for the batch.\n",
    "        # tf.nce_loss automatically draws a new sample of the negative labels each\n",
    "        # time we evaluate the loss.\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.nce_loss(weights=nce_weights,\n",
    "                           biases=nce_biases,\n",
    "                           labels=train_labels,\n",
    "                           inputs=embed,\n",
    "                           num_sampled=num_sampled,\n",
    "                           num_classes=vocabulary_size)\n",
    "                              )\n",
    "        # Construct the SGD optimizer using a learning rate of 1.0.\n",
    "        optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    " \n",
    "        # Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "        normalized_embeddings = embeddings / norm\n",
    "        valid_embeddings = tf.nn.embedding_lookup(\n",
    "            normalized_embeddings, valid_dataset)\n",
    "        similarity = tf.matmul(\n",
    "            valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "        # Add variable initializer.\n",
    "        init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_embeddings(embeddings):\n",
    "    with open(\"../data/emoji2vec/e2v_full.txt\", \"w\") as file:\n",
    "        for i in range(len(emojis)):\n",
    "            file.write(emojis[i])\n",
    "            file.write('\\t')\n",
    "            for j in range(300):\n",
    "                file.write(str(final_embeddings[i][j]))\n",
    "                file.write(' ')\n",
    "            file.write('\\n')\n",
    "    with open(\"../data/word2vec/w2v_full.txt\", \"w\") as file:\n",
    "        for i in range(words_voc_size):\n",
    "            file.write(reverse_dictionary[i+len(emojis)])\n",
    "            file.write('\\t')\n",
    "            for j in range(300):\n",
    "                file.write(str(final_embeddings[i+len(emojis)][j]))\n",
    "                file.write(' ')\n",
    "            file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Begin training.\n",
    "num_steps = 10000001\n",
    "config = tf.ConfigProto(\n",
    "    device_count = {'GPU': 0}\n",
    ")\n",
    "with tf.Session(graph=graph, config=config) as session:\n",
    "    # We must initialize all variables before we use them.\n",
    "#     saver = tf.train.Saver()\n",
    "#     saver.restore(session, 'session'+\"/model.ckpt\")\n",
    "    \n",
    "    init.run()\n",
    "    print('Initialized')\n",
    "\n",
    "    average_loss = 0\n",
    "    for step in xrange(num_steps):\n",
    "        batch_inputs, batch_labels = generate_batch()\n",
    "        \n",
    "        feed_dict = {\n",
    "            train_inputs: batch_inputs, \n",
    "            train_labels: batch_labels, \n",
    "            #embeddings_words: np.array(full_embeddings)\n",
    "        }\n",
    "        \n",
    "        # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "        if step % 50000 == 0:\n",
    "            sim = session.run(similarity, feed_dict=feed_dict)\n",
    "            for i in xrange(valid_size):\n",
    "                valid_word = reverse_dictionary[valid_examples[i]]\n",
    "                top_k = 8  # number of nearest neighbors\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "                log_str = 'Nearest to %s:' % valid_word\n",
    "                for k in xrange(top_k):\n",
    "                    close_word = reverse_dictionary[nearest[k]]\n",
    "                    log_str = '%s %s,' % (log_str, close_word)\n",
    "                print(log_str)\n",
    "        if step % 150000 == 0: \n",
    "            final_embeddings = session.run(normalized_embeddings, feed_dict=feed_dict)\n",
    "            write_embeddings(final_embeddings)\n",
    "            \n",
    "            pk.dump(reverse_dictionary, open('../data/reverse_dictionary.txt', 'wb'))\n",
    "            \n",
    "            saver = tf.train.Saver()\n",
    "            # Save a checkpoint with the trained model\n",
    "            saver.save(session, '../data/session'+\"/model.ckpt\")\n",
    "            \n",
    "        _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        average_loss += loss_val\n",
    "\n",
    "        if step % 2000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss /= 2000\n",
    "            # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "            print('Average loss at step ', step, ': ', average_loss)\n",
    "            average_loss = 0\n",
    "    final_embeddings = session.run(normalized_embeddings, feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_embeddings[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
