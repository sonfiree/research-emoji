{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are trying to learn e2v embeddings using emoji descriptions and pre-trained Google w2v for words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "import pickle as pk\n",
    "import gensim.models as gs\n",
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "# Internal dependencies\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from model import Emoji2Vec\n",
    "from trainer import Trainer\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "\n",
    "from tfnn.layers import *\n",
    "import tfnn\n",
    "import math\n",
    "import mt.bleu\n",
    "import sys\n",
    "import numpy as np\n",
    "import collections\n",
    "from mt.strutils import tokenize\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "minibatch_len = 4\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v = gs.KeyedVectors.load_word2vec_format(\"../data/word2vec/GoogleNews-vectors-negative300.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alphabet = 'abcdefghijklmnopqrstuvwxyz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counts = {}\n",
    "for word, item in w2v.vocab.items():\n",
    "    counts[word] = item.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lower_counts = {}\n",
    "lower_to_origin = {}\n",
    "for word, value in counts.items():\n",
    "    if np.any([w not in alphabet for w in word.lower()]):\n",
    "        continue\n",
    "            \n",
    "    changed = False\n",
    "    if word.lower() in lower_counts:\n",
    "        lower_counts[word.lower()] = max(lower_counts[word.lower()], value)\n",
    "        changed = True\n",
    "    else:\n",
    "        lower_counts[word.lower()] = value\n",
    "    if word.lower() not in lower_to_origin or changed:\n",
    "        lower_to_origin[word.lower()] = word     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "freq_and_word = lambda item: item[::-1]\n",
    "most_frequent = sorted(lower_counts.items(), key=freq_and_word, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "most_frequent = most_frequent[:80000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec = {}\n",
    "for word, _ in most_frequent:\n",
    "    word2vec[word.lower()] = w2v[lower_to_origin[word]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_data(datafile):\n",
    "    \n",
    "    def normalize_word(word):\n",
    "        word = word.lower()\n",
    "        try:\n",
    "            while word[0] not in alphabet:\n",
    "                word = word[1:]\n",
    "            while word[-1] not in alphabet:\n",
    "                word = word[:-1]\n",
    "        except:\n",
    "            return None\n",
    "        if word not in word2vec:\n",
    "            print(word)\n",
    "            return None\n",
    "        return word\n",
    "    \n",
    "    smiles_to_words = {} \n",
    "    emojis = []\n",
    "    words_from_emojis = []\n",
    "    with open(datafile, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            try:\n",
    "                em, phrase, truth = line.rstrip().split('\\t')\n",
    "            except:\n",
    "                continue\n",
    "            phrase = phrase.lower()\n",
    "            new_words = [normalize_word(new_word) for new_word in phrase.split() if not normalize_word(new_word) is None]\n",
    "            if em in smiles_to_words:\n",
    "                smiles_to_words[em].extend(new_words)\n",
    "            else:\n",
    "                smiles_to_words[em] = new_words\n",
    "            emojis.append(em)\n",
    "            words_from_emojis.extend(new_words)\n",
    "    for key, value in smiles_to_words.items():\n",
    "        normalized_words = []\n",
    "        for word in list(set(value)):\n",
    "            norm_word = normalize_word(word)\n",
    "            if not norm_word is None:\n",
    "                normalized_words.append(norm_word)\n",
    "        smiles_to_words[key] = normalized_words\n",
    "    emojis = list(set(emojis))\n",
    "    words_from_emojis = list(set(words_from_emojis))\n",
    "    return smiles_to_words, emojis, words_from_emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "smiles_to_words, emojis, words_from_emojis = make_data('../data/emojipedia_positive.txt')\n",
    "words = list(word2vec.keys())\n",
    "emojis_to_ind = {emojis[i]: i+80000 for i in range(len(emojis))}\n",
    "words_to_ind = {most_frequent[i][0].lower(): i for i in range(len(most_frequent))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reverse_dictionary = []\n",
    "reverse_dictionary.extend([most_frequent[i][0].lower() for i in range(80000)])\n",
    "reverse_dictionary.extend(emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emoji_voc_size = len(emojis)\n",
    "words_voc_size = 80000\n",
    "vocabulary_size = emoji_voc_size + words_voc_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "smiles = list(smiles_to_words.keys())\n",
    "smile_ind = 0\n",
    "\n",
    "def get_minibatch():\n",
    "    global smile_ind\n",
    "    minibatch = []\n",
    "    targets_to_awoid = []\n",
    "    \n",
    "    curr_words = smiles_to_words[smiles[smile_ind]]\n",
    "    while len(curr_words) < 1:\n",
    "        smile_ind += 1\n",
    "        smile_ind %= len(smiles)\n",
    "        curr_words = smiles_to_words[smiles[smile_ind]]\n",
    "    for i in range(minibatch_len):\n",
    "        smile = smiles[smile_ind]\n",
    "        word_ind = random.randint(0, len(curr_words)-1)\n",
    "        while word_ind in targets_to_awoid:\n",
    "            word_ind += 1\n",
    "            word_ind %= len(curr_words)\n",
    "        word = curr_words[word_ind]\n",
    "        targets_to_awoid.append(word)\n",
    "        if i < min(minibatch_len, len(smiles_to_words[smiles[smile_ind]])) / 2:\n",
    "            minibatch.append((emojis_to_ind[smile], words_to_ind[word])) \n",
    "        else:\n",
    "            #break\n",
    "            minibatch.append((words_to_ind[word], emojis_to_ind[smile]))\n",
    "        if len(targets_to_awoid) == len(curr_words):\n",
    "            break\n",
    "    smile_ind += 1\n",
    "    if smile_ind >= len(smiles):\n",
    "        smile_ind = 0\n",
    "    return minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_batch():\n",
    "    batch = []\n",
    "    while len(batch) < batch_size:\n",
    "        batch.extend(get_minibatch())\n",
    "    random.shuffle(batch)\n",
    "    return [batch[i][0] for i in range(batch_size)], [[batch[i][1]] for i in range(batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_size = 300\n",
    "valid_size = 32     # Random set of words to evaluate similarity on.\n",
    "valid_examples = list(np.random.choice(len(smiles), valid_size // 2, replace=False))\n",
    "valid_examples.extend([x + 80000 for x in valid_examples])\n",
    "num_sampled = 32    # Number of negative examples to sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unk_vector = [np.mean([word2vec[most_frequent[j][0]][i] for j in range(80000)]) for i in range(300)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_embeddings = []\n",
    "for i, word in enumerate(most_frequent):\n",
    "    full_embeddings.append(word2vec[most_frequent[i][0]])\n",
    "for emoji in emojis:\n",
    "    full_embeddings.append(unk_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(full_embeddings).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data.\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "    #embeddings_words = tf.placeholder(tf.float32, shape=[80000+len(emojis), embedding_size])\n",
    "    \n",
    "    with tf.device('/cpu:0'):\n",
    "        embeddings = tf.Variable(np.array(full_embeddings), dtype=tf.float32)\n",
    "        embeddings = tf.Variable(\n",
    "            tf.random_uniform([len(full_embeddings), embedding_size], -1.0, 1.0))\n",
    "        choice = [2 for i in range(80000)]\n",
    "        choice.extend([0 for i in range(len(emojis))])\n",
    "        choice = tf.constant(choice)\n",
    "        embeddings = tf.where(tf.less(choice, [1]), embeddings, embeddings_words)\n",
    "        embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "        \n",
    "        # Construct the variables for the NCE loss\n",
    "        nce_weights = tf.Variable(\n",
    "            tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                                stddev=1.0 / math.sqrt(embedding_size)))\n",
    "        nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "        \n",
    "        # Compute the average NCE loss for the batch.\n",
    "        # tf.nce_loss automatically draws a new sample of the negative labels each\n",
    "        # time we evaluate the loss.\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.nce_loss(weights=nce_weights,\n",
    "                           biases=nce_biases,\n",
    "                           labels=train_labels,\n",
    "                           inputs=embed,\n",
    "                           num_sampled=num_sampled,\n",
    "                           num_classes=vocabulary_size)\n",
    "                              )\n",
    "        \n",
    "        # Construct the SGD optimizer using a learning rate of 1.0.\n",
    "        optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    " \n",
    "        # Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "        normalized_embeddings = embeddings / norm\n",
    "        valid_embeddings = tf.nn.embedding_lookup(\n",
    "            normalized_embeddings, valid_dataset)\n",
    "        similarity = tf.matmul(\n",
    "            valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "        # Add variable initializer.\n",
    "        init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Begin training.\n",
    "num_steps = 10000001\n",
    "\n",
    "config = tf.ConfigProto(\n",
    "    device_count = {'GPU': 1}\n",
    ")\n",
    "with tf.Session(graph=graph, config=config) as session:\n",
    "    # We must initialize all variables before we use them.\n",
    "    init.run()\n",
    "    print('Initialized')\n",
    "\n",
    "    average_loss = 0\n",
    "    for step in xrange(num_steps):\n",
    "        batch_inputs, batch_labels = generate_batch()\n",
    "        \n",
    "        feed_dict = {\n",
    "            train_inputs: batch_inputs, \n",
    "            train_labels: batch_labels, \n",
    "            embeddings_words: np.array(full_embeddings)\n",
    "        }\n",
    "        \n",
    "        # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "        if step % 50000 == 0:\n",
    "            sim = session.run(similarity, feed_dict=feed_dict)\n",
    "            for i in xrange(valid_size):\n",
    "                valid_word = reverse_dictionary[valid_examples[i]]\n",
    "                top_k = 8  # number of nearest neighbors\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "                log_str = 'Nearest to {}:'.format(valid_word)\n",
    "                for k in xrange(top_k):\n",
    "                    close_word = reverse_dictionary[nearest[k]]\n",
    "                    log_str = '%s %s,' % (log_str, close_word)\n",
    "                print(log_str)\n",
    "            final_embeddings = session.run(normalized_embeddings, feed_dict=feed_dict)\n",
    "            pk.dump(final_embeddings, open('e2v_smiles_only', 'wb'))\n",
    "\n",
    "        # We perform one update step by evaluating the optimizer op (including it\n",
    "        # in the list of returned values for session.run()\n",
    "        _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        average_loss += loss_val\n",
    "\n",
    "        if step % 2000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss /= 2000\n",
    "            # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "            print('Average loss at step ', step, ': ', average_loss)\n",
    "            average_loss = 0\n",
    "    final_embeddings = session.run(normalized_embeddings, feed_dict=feed_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
