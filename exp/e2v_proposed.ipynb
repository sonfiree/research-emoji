{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for training emoji embeddings as proposed in this paper: https://arxiv.org/abs/1609.08359"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "import pickle as pk\n",
    "import gensim.models as gs\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "\n",
    "# Internal dependencies\n",
    "from model import Emoji2Vec\n",
    "from trainer import Trainer\n",
    "from batcher import BatchNegSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"word2vec_file\": \"../data/word2vec/GoogleNews-vectors-negative300.bin\",\n",
    "    \"emoj2vec_file\": \"../data/emoji2vec/emoji2vec_proposed.bin\",\n",
    "    \"dimension\": 300,\n",
    "    \"train_data_true\": \"../data/emojipedia/emojipedia_positive.txt\",\n",
    "    \"train_data_false\": \"../data/emojipedia/emojipedia_negative.txt\",\n",
    "#     \"dev_data\": \"../data/training/dev.txt\",\n",
    "#     \"test_data\": \"../data/training/test.txt\",\n",
    "    \"ind_to_emoj_file\": \"../data/proposed/ind_to_emoj.pk\",\n",
    "    \"ind_to_phr_file\": \"..data/proposed/ind_to_phr.pk\",\n",
    "    \"embeddings_file\": \"../data/proposed/phrase_embeddings.pk\",\n",
    "    \"model_path\": \"../data/proposed/model\",\n",
    "    \n",
    "    \n",
    "    \"item\": \"happy face\",\n",
    "    \"top_n\": 8,\n",
    "}\n",
    "\n",
    "# hyperparams for model\n",
    "hp = {\n",
    "    \"in_dim\": 300,\n",
    "    \"out_dim\": 300,\n",
    "    \"max_epochs\": 10,\n",
    "    \"batch_size\": 8,\n",
    "    \"neg_ratio\": 1,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"dropout\": 0.0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pre-trained Google word2vec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print('reading embedding data from: ' + args[\"word2vec_file\"])\n",
    "w2v = gs.KeyedVectors.load_word2vec_format(args[\"word2vec_file\"], binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alphabet = 'abcdefghigklmnopqrstuvwxyz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make ind_to_emoji, ind_to_phrase dectionaries and sums of word embeddings for each phrase in data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_data(args):\n",
    "\n",
    "    def phrase_vec_model(item):\n",
    "        tokens = item.split(' ')\n",
    "        phr_sum = np.zeros(args[\"dimension\"], np.float32)\n",
    "\n",
    "        for token in tokens:\n",
    "            if token in w2v:\n",
    "                phr_sum += w2v[token]\n",
    "        \n",
    "        return phr_sum\n",
    "\n",
    "    phrase_vector_sums = dict()\n",
    "    ind_to_emoj = []\n",
    "    ind_to_phr = []\n",
    "    for file in [args[\"train_data_true\"], args[\"train_data_false\"]]:\n",
    "        with open(file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            for i, line in enumerate(lines):\n",
    "                if i % 10000 == 0:\n",
    "                    print(i/10000)\n",
    "                try:\n",
    "                    em, phrase, truth = line.rstrip().split('\\t')\n",
    "                except Exception as e:\n",
    "                    print(line.rstrip().split('\\t'))\n",
    "                    continue\n",
    "                phrase = phrase.lower()\n",
    "                try:\n",
    "                    while phrase[0] not in alphabet:\n",
    "                        phrase = phrase[1:]\n",
    "                    while phrase[-1] not in alphabet:\n",
    "                        phrase = phrase[:-1]\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "                phrase_vector_sums[phrase] = phrase_vec_model(phrase)\n",
    "                if em not in ind_to_emoj:\n",
    "                    ind_to_emoj.append(em)\n",
    "                if phrase not in ind_to_phr:\n",
    "                    ind_to_phr.append(phrase)\n",
    "\n",
    "    pk.dump(ind_to_emoj, open(args[\"ind_to_emoj_file\"], 'wb'))\n",
    "    pk.dump(ind_to_phr, open(args[\"ind_to_phr_file\"], 'wb'))\n",
    "    pk.dump(phrase_vector_sums, open(args[\"embeddings_file\"], 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "2.0\n",
      "3.0\n",
      "4.0\n",
      "5.0\n",
      "6.0\n",
      "7.0\n",
      "8.0\n",
      "9.0\n",
      "10.0\n",
      "11.0\n",
      "12.0\n",
      "13.0\n",
      "14.0\n",
      "15.0\n",
      "16.0\n",
      "17.0\n",
      "18.0\n",
      "19.0\n",
      "20.0\n",
      "21.0\n",
      "22.0\n",
      "23.0\n",
      "24.0\n",
      "25.0\n",
      "26.0\n",
      "27.0\n",
      "28.0\n",
      "29.0\n",
      "30.0\n",
      "31.0\n",
      "32.0\n",
      "33.0\n",
      "34.0\n",
      "35.0\n",
      "36.0\n",
      "37.0\n",
      "38.0\n",
      "39.0\n",
      "40.0\n",
      "41.0\n",
      "42.0\n",
      "43.0\n",
      "44.0\n",
      "45.0\n",
      "46.0\n",
      "47.0\n",
      "48.0\n",
      "49.0\n",
      "50.0\n",
      "51.0\n",
      "52.0\n",
      "53.0\n",
      "54.0\n",
      "55.0\n",
      "56.0\n",
      "57.0\n",
      "58.0\n",
      "59.0\n",
      "60.0\n",
      "61.0\n",
      "62.0\n",
      "63.0\n",
      "64.0\n",
      "65.0\n",
      "66.0\n",
      "67.0\n",
      "68.0\n",
      "69.0\n",
      "70.0\n",
      "71.0\n",
      "72.0\n",
      "73.0\n",
      "74.0\n",
      "75.0\n",
      "76.0\n",
      "77.0\n",
      "78.0\n",
      "79.0\n",
      "80.0\n",
      "81.0\n",
      "82.0\n",
      "83.0\n",
      "84.0\n",
      "85.0\n",
      "86.0\n",
      "87.0\n"
     ]
    }
   ],
   "source": [
    "process_data(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data made using process_data function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ind_to_phr = pk.load(open(args[\"ind_to_phr_file\"], 'rb'))\n",
    "ind_to_emoj = pk.load(open(args[\"ind_to_emoj_file\"], 'rb'))\n",
    "\n",
    "phr_to_ind = {v: k for k, v in enumerate(ind_to_phr)}\n",
    "emoj_to_ind = {v: k for k, v in enumerate(ind_to_emoj)}\n",
    "\n",
    "phrase_vector_sums = pk.load(open(args[\"embeddings_file\"], 'rb'))\n",
    "\n",
    "embeddings_array = np.zeros(shape=[len(ind_to_phr), 300], dtype=np.float32)\n",
    "for ind, phr in enumerate(ind_to_phr):\n",
    "    embeddings_array[ind] = phrase_vector_sums[phr]\n",
    "\n",
    "emoj_to_ind = {v: k for k, v in enumerate(ind_to_emoj)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def restore_sess():\n",
    "    ops.reset_default_graph()\n",
    "    model = Emoji2Vec(hp, len(ind_to_emoj), embeddings_array=None, use_embeddings=False)\n",
    "\n",
    "    session = tf.Session()\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(session, args[\"model_path\"]+\"/model.ckpt\")\n",
    "    \n",
    "    w2v, e2v = from_2vec_paths(args[\"word2vec_file\"], args[\"emoj2vec_file\"])\n",
    "    return w2v, e2v, session, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_batch(datafile):\n",
    "    rows = list()\n",
    "    cols = list()\n",
    "    targets = list()\n",
    "    with open(datafile, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        batch = []\n",
    "        for line in lines:\n",
    "            em, phrase, truth = line.rstrip().split('\\t')\n",
    "            phrase = phrase.lower()\n",
    "            try:\n",
    "                while phrase[0] not in alphabet:\n",
    "                    phrase = phrase[1:]\n",
    "                while phrase[-1] not in alphabet:\n",
    "                    phrase = phrase[:-1]\n",
    "            except Exception as e:\n",
    "                continue\n",
    "            batch.append((phrase, em, truth))\n",
    "            cols.append(emoj_to_ind[em])\n",
    "            rows.append(phr_to_ind[phrase])\n",
    "            targets.append(1 if truth == 'True' else 0)\n",
    "    return batch, (rows, cols, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def TRAIN(args):\n",
    "    ops.reset_default_graph()\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # Create the model based on the given model parameters\n",
    "    print(str.format('Training: k={}, batch={}, epochs={}, ratio={}, dropout={}', hp[\"out_dim\"],\n",
    "                         hp[\"batch_size\"], hp[\"max_epochs\"], hp[\"neg_ratio\"], hp[\"dropout\"]))\n",
    "    model = Emoji2Vec(hp=hp, num_emoji=len(ind_to_emoj), embeddings_array=embeddings_array)\n",
    "\n",
    "    dsets = {\n",
    "        'train_true': load_batch(args[\"train_data_true\"])[0],\n",
    "        'train_false': load_batch(args[\"train_data_false\"])[0], \n",
    "        #'dev': load_batch(args[\"dev_data\"])[0]\n",
    "        }\n",
    "\n",
    "    sess = tf.Session()\n",
    "\n",
    "    # corpus is the body from which we sample\n",
    "    corpus = BatchNegSampler(phr_to_ind, emoj_to_ind, hp[\"batch_size\"], \n",
    "                             hp[\"neg_ratio\"], dsets[\"train_true\"], dsets[\"train_false\"])\n",
    "\n",
    "    model.train(corpus, session=sess, datasets=dsets)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    # Save a checkpoint with the trained model\n",
    "    saver.save(sess, args[\"model_path\"]+\"/model.ckpt\")\n",
    "\n",
    "    # Generate the gensim structures\n",
    "    e2v = model.create_gensim_files(sess=sess, model_folder=args[\"model_path\"], ind2emoj=ind_to_emoj,\n",
    "                                    out_dim=hp[\"out_dim\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
